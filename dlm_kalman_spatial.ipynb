{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import pystan\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.interpolate as interpolate\n",
    "import netCDF4\n",
    "from models.stan_dlm_models import *\n",
    "import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DLM model: this translates the stan model in stan_dlm_models.py into C++ and compiles it\n",
    "# Note: this step may take a few minutes\n",
    "model_kalman_ar1 = pystan.StanModel(model_code=code_kalman_ar1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data \n",
    "# You can do this however you like but the end result must be the following variables loaded in:\n",
    "# N (int) = number of time-steps in the time-series\n",
    "# d float[N] = the data time-series (IMPORTANT: NaNs are NOT allowed, for missing values please set those data to zero and set\n",
    "# the corresponding error for those data points to something very large, like 10^20)\n",
    "# s float[N] = std-deviation error-bars on each data point (set error bars for missing values to a large number; see above)\n",
    "\n",
    "# Import data from a netCDF\n",
    "data = netCDF4.Dataset('data/BASIC_V1_2017_lotus_seascyc_gcsw2017_fac2.nc')\n",
    "\n",
    "# Extract time, pressure and latitude variables from the netCDF\n",
    "T = data['time'][:]\n",
    "P = data['pressure'][:]\n",
    "L = data['latitude'][:]\n",
    "\n",
    "# How many time steps are there? (ie how long is the time-series)\n",
    "N = len(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regressors\n",
    "\n",
    "# Again you can do this however you want, but the result must be the following variables loaded into python\n",
    "# nreg (int) = number of regressors\n",
    "# regressors float[N, nreg] = 2d array with each column representing one regressor over the times corresponding to\n",
    "# the data to be analysed\n",
    "\n",
    "# NOTE: We also mean subtract and rescale all regressors in the same way as for the data so they have dymanic range \n",
    "# of 1 \n",
    "\n",
    "# IMPORTANT: Missing values/NaNs are NOT supported, please interpolate missing values or similar so they are all \n",
    "# real valued\n",
    "\n",
    "# ENSO\n",
    "y = np.loadtxt('regressors/ENSO_MEI_ENSO_1950_201802.dat')\n",
    "t = np.loadtxt('regressors/ENSOTime_MEI_ENSO_1950_201802.dat')\n",
    "y = y - np.mean(y)\n",
    "y = y/(max(y) - min(y))\n",
    "Y = interpolate.InterpolatedUnivariateSpline(t, y)\n",
    "enso = Y(T) # This extacts the regressor at the times you have in your dataset, interpolated in case the time grid is different to the actual data\n",
    "\n",
    "# Solar\n",
    "y = np.loadtxt('regressors/Flux_F30_monthly_195111_201803_absolute.dat')\n",
    "t = np.loadtxt('regressors/Time_F30_monthly_195111_201803_absolute.dat')\n",
    "y = y - np.mean(y)\n",
    "y = y/(max(y) - min(y))\n",
    "Y = interpolate.InterpolatedUnivariateSpline(t, y)\n",
    "solar = Y(T)\n",
    "\n",
    "# QBO30\n",
    "y = np.loadtxt('regressors/multi_qbo30_1953_2018.dat')\n",
    "t = np.loadtxt('regressors/multi_qbotime_1953_2018.dat')\n",
    "y = y - np.mean(y)\n",
    "y = y/(max(y) - min(y))\n",
    "Y = interpolate.InterpolatedUnivariateSpline(t, y)\n",
    "qbo30 = Y(T) \n",
    "\n",
    "# QBO50\n",
    "y = np.loadtxt('regressors/multi_qbo50_1953_2018.dat')\n",
    "t = np.loadtxt('regressors/multi_qbotime_1953_2018.dat')\n",
    "y = y - np.mean(y)\n",
    "y = y/(max(y) - min(y))\n",
    "Y = interpolate.InterpolatedUnivariateSpline(t, y)\n",
    "qbo50 = Y(T) # This extacts the regressor at the times you have in your dataset, interpolated in case the time grid is different\n",
    "\n",
    "# SAOD\n",
    "saod_data = netCDF4.Dataset('regressors/sad_1979_2017_10deg_60S_60N_missNans.nc')\n",
    "# ***RESCALING WILL BE DONE ON THE FLY AS IT IS LATITUDE DEPENDENT***\n",
    "\n",
    "# How many regressors?\n",
    "nreg = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many MCMC chains/samples to do\n",
    "n_chains = 4\n",
    "warmup = 1000\n",
    "iterations = 3000\n",
    "n_samples = n_chains*(iterations-warmup)\n",
    "\n",
    "# Make the netcdf file for saving the data\n",
    "results = netCDF4.Dataset('results/results.nc', 'w')\n",
    "\n",
    "# Create dimensions\n",
    "results.createDimension('pressure', len(P))\n",
    "results.createDimension('latitude', len(L))\n",
    "results.createDimension('time', len(T))\n",
    "results.createDimension('nsamples', n_samples)\n",
    "results.createDimension('nreg', nreg)\n",
    "\n",
    "# Create variables\n",
    "lats = results.createVariable('latitude', int, ('latitude',), zlib=True)\n",
    "pressures = results.createVariable('pressure', 'float', ('pressure',), zlib=True)\n",
    "times = results.createVariable('time', int, ('time',), zlib=True)\n",
    "trend = results.createVariable('trend', float, ('nsamples','pressure','latitude', 'time'), zlib=True)\n",
    "seasonal = results.createVariable('seasonal', float, ('nsamples','pressure','latitude', 'time'), zlib=True)\n",
    "slope = results.createVariable('slope', float, ('nsamples','pressure','latitude','time'), zlib=True)\n",
    "residuals = results.createVariable('residuals', float, ('nsamples','pressure','latitude','time'), zlib=True)\n",
    "regressor_coefficients = results.createVariable('regressor_coefficients', float, ('nsamples','pressure','latitude','nreg'), zlib=True)\n",
    "sigma_trend = results.createVariable('sigma_trend', float, ('nsamples','pressure','latitude'), zlib=True)\n",
    "sigma_seas = results.createVariable('sigma_seas', float, ('nsamples','pressure','latitude'), zlib=True)\n",
    "sigma_AR = results.createVariable('sigma_AR', float, ('nsamples','pressure','latitude'), zlib=True)\n",
    "rho_AR = results.createVariable('rho_AR', float, ('nsamples','pressure','latitude'), zlib=True)\n",
    "\n",
    "# Assign variables\n",
    "lats[:] = L\n",
    "times[:] = T\n",
    "pressures[:] = P\n",
    "\n",
    "# Set up the progress bar\n",
    "pbar = tqdm.tqdm_notebook(total = len(P)*len(L), desc = \"press/lats\")\n",
    "\n",
    "# Loop over pressures and latitudes\n",
    "for pressure in range(len(P)):\n",
    "    for latitude in range(len(L)):\n",
    "\n",
    "        # Set the data and initialization of parameters that are fed into the DLM\n",
    "\n",
    "        # Pick out a pressure and latitude panel: this is the \"data\" time-series from the netCDF\n",
    "        d = data['o3'][:, pressure, latitude]\n",
    "\n",
    "        # Extract the error-bars on the time-series from the netCDF\n",
    "        s = data['o3_sigma'][:, pressure, latitude]\n",
    "\n",
    "        # IMPORTANT: The DLM is set-up to run on mean subtracted data re-scaled to have a dynamic range of unity\n",
    "        # NB: We must re-scale the data here and re-scale back to the original units after the DLM run, look out for this later\n",
    "\n",
    "        # Mean subtract and re-scale the data so it has dynamic range of unity, \n",
    "        # Apply same scaling transform to the std-dev error bars\n",
    "        scale = max(d) - min(d)\n",
    "        mean = np.mean(d)\n",
    "        d = d - mean\n",
    "        d = d/scale\n",
    "        s = s/scale\n",
    "        \n",
    "        # Set the SAOD regressor depending on the latitude\n",
    "        y = saod_data['saod_strat_column'][:, latitude]\n",
    "        t = saod_data['time'][:]\n",
    "        y = y - np.mean(y)\n",
    "        y = y/(max(y) - min(y))\n",
    "        Y = interpolate.InterpolatedUnivariateSpline(t, y)\n",
    "        saod = Y(T)\n",
    "\n",
    "        # Regressors\n",
    "        regressors = np.column_stack([enso, solar, qbo30, qbo50, saod]) # Stack of all the regressors together in a 2d array\n",
    "\n",
    "        # Data: this is a dictionary of all of the data/inputs that the DLM model needs (descriptions below)\n",
    "        data = {\n",
    "                            'd':d, # float[N] data vector\n",
    "                            's':s, # float[N] std-dev error bars\n",
    "                            'N':N, # (int) number of time-steps in the time-series\n",
    "                            'nreg':nreg, # (int) number of regressors\n",
    "                            'regressors':regressors, # float[N, nreg] the regressors\n",
    "                            'S':10., # prior variance on the regression coefficients (priors are zero mean Gaussian with variance S)\n",
    "                            'sigma_trend_prior':1e-4, # std-dev of the half-Gaussian prior on sigma_trend that controls how wiggly the trend can be\n",
    "                            'sigma_seas_prior':0.01, # std-dev of the half-Gaussian prior on sigma_seas that controls how dynamic the seaonal cycle can be\n",
    "                            'sigma_AR_prior':0.5 # std-dev of the half_Gaussian prior on the AR1 process std-dev \n",
    "                        }\n",
    "\n",
    "        # Initialization: Initial guess values for the hyper-parameters\n",
    "        initial_state = {\n",
    "                         'sigma_trend':0.0001,\n",
    "                         'sigma_seas':0.001,\n",
    "                         'sigma_AR':0.01,\n",
    "                         'rhoAR':0.1,\n",
    "                        }\n",
    "        \n",
    "        # Run the model\n",
    "        fit = model_kalman_ar1.sampling(data=data, iter=iterations, warmup=warmup, chains=n_chains, init = [initial_state for i in range(n_chains)], verbose=True, pars=('sigma_trend', 'sigma_seas', 'sigma_AR', 'rhoAR', 'trend', 'slope', 'beta', 'seasonal', 'residuals'))\n",
    "\n",
    "        # Extract and re-scale the key bits from the DLM run\n",
    "        \n",
    "        # Trend\n",
    "        trend[:, pressure, latitude, :] = fit.extract()['trend'][:,:]*scale + mean\n",
    "\n",
    "        # Gradient of the DLM trend\n",
    "        slope[:, pressure, latitude, :] = fit.extract()['slope'][:,:]*scale\n",
    "\n",
    "        # Seasonal cycle\n",
    "        seasonal[:, pressure, latitude, :] = fit.extract()['seasonal'][:,:]*scale\n",
    "\n",
    "        # Residuals\n",
    "        residuals[:, pressure, latitude, :] = fit.extract()['residuals'][:,:]*scale\n",
    "\n",
    "        # Regressor coefficients (also need re-scaling back to data units)\n",
    "        regressor_coefficients[:, pressure, latitude, :] = fit.extract()['beta'][:,:]*scale\n",
    "\n",
    "        # DLM hyper parameters\n",
    "        sigma_trend[:, pressure, latitude] = fit.extract()['sigma_trend']\n",
    "        sigma_seas[:, pressure, latitude] = fit.extract()['sigma_seas']\n",
    "        sigma_AR[:, pressure, latitude] = fit.extract()['sigma_AR']*scale # Also needs re-scaling back to data units\n",
    "        rho_AR[:, pressure, latitude] = fit.extract()['sigma_AR']\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "        \n",
    "results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
